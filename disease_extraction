import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from transformers import pipeline
import json
import os
import time
from tqdm import tqdm
import re

# Set up paths to match the project structure
base_dir = os.getcwd()
data_dir = os.path.join(base_dir, "Dataset", "data")
output_dir = os.path.join(base_dir, "Dataset", "data")
os.makedirs(output_dir, exist_ok=True)

# Define a function to clean disease mentions
def clean_disease(disease):
    # Remove prepositions or trailing punctuation
    disease = disease.strip().lower()
    
    # Remove leading prepositions
    disease = re.sub(r'^(for|with|in|by|on|of|and|the|a|an)\s+', '', disease)
    
    # Remove trailing prepositions and conjunctions
    disease = re.sub(r'\s+(for|with|in|by|on|of|and|the|a|an)$', '', disease)
    
    # Remove common stopwords anywhere in the text
    disease = re.sub(r'\s+(is|are|was|were|been|be|as|at|from|to|in|with|for|by|on|of)\s+', ' ', disease)
    
    # Remove trailing commas and other punctuation
    disease = re.sub(r'[^\w\s-]', '', disease)
    
    # Remove extra whitespace
    disease = re.sub(r'\s+', ' ', disease).strip()
    
    return disease.strip().title()  # Normalize to Title Case

print(f"Loading data from {data_dir}")

# Load all data files
df_train = pd.read_csv(os.path.join(data_dir, "train.csv"))
df_val = pd.read_csv(os.path.join(data_dir, "val.csv"))
test_path = os.path.join(data_dir, "test.csv")
if os.path.exists(test_path):
    df_test = pd.read_csv(test_path)
    df_all = pd.concat([df_train, df_val, df_test], ignore_index=True)
else:
    df_all = pd.concat([df_train, df_val], ignore_index=True)

print(f"Loaded {len(df_all)} total abstracts")

# Load NER model
print("Loading ClinicalBERT model...")
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Set up NER pipeline
print("Setting up NER pipeline...")
ner = pipeline("ner", 
              model=model_name, 
              tokenizer=tokenizer,
              aggregation_strategy="first")

# Define cancer-specific keywords
cancer_keywords = [
    "cancer", "tumor", "carcinoma", "leukemia", "lymphoma", 
    "sarcoma", "melanoma", "neoplasm", "malignancy", "metastasis",
    "oncology", "carcinogenesis", "oncogene", "adenoma", "adenocarcinoma",
    "glioma", "glioblastoma", "myeloma", "blastoma", "mesothelioma",
    "malignant", "metastatic", "neoplastic", "cancerous"
]

# Filter for cancer abstracts
df_cancer = df_all[df_all['label'] == 1]  # Directly filter where label is 1 (cancer)
print(f"Filtered {len(df_cancer)} cancer abstracts out of {len(df_all)} total")

# Process abstracts with NER
extracted = []
print(f"Processing {len(df_cancer)} cancer abstracts...")
start_time = time.time()

for i, row in tqdm(df_cancer.iterrows(), total=len(df_cancer)):
    abstract = str(row['abstract']) if pd.notna(row['abstract']) else ""
    
    # Skip empty abstracts
    if not abstract.strip():
        continue
    
    # Process in chunks if abstract is very long (ClinicalBERT context length limit is 512)
    if len(abstract) > 512:
        chunks = [abstract[i:i+512] for i in range(0, len(abstract), 512)]
        all_entities = []
        for chunk in chunks:
            chunk_entities = ner(chunk)
            all_entities.extend(chunk_entities)
    else:
        all_entities = ner(abstract)
    
    # Extract diseases
    diseases = set()
    for entity in all_entities:
        entity_word = entity.get("word", "")
        
        # Clean the disease name
        if entity_word:
            cleaned_disease = clean_disease(entity_word)
            if cleaned_disease and any(keyword in cleaned_disease.lower() for keyword in cancer_keywords):
                if len(cleaned_disease.split()) >= 2:  # Only include multi-word terms
                    diseases.add(cleaned_disease)
    
    # Only include abstracts where diseases were found
    if diseases:
        extracted.append({
            "abstract_id": str(row['id']),
            "extracted_diseases": list(diseases)
        })
    
    # Print occasional progress
    if (i+1) % 25 == 0:
        elapsed = time.time() - start_time
        rate = (i+1) / elapsed
        print(f"Processed {i+1} abstracts ({rate:.2f} abstracts/sec)")

# Save to output file
output_file = os.path.join(output_dir, "cancer_extraction_combined.json")
with open(output_file, "w") as f:
    json.dump(extracted, f, indent=2)

print(f"\n✅ Processed {len(df_cancer)} abstracts, found cancer mentions in {len(extracted)}")
print(f"✅ All cancer disease mentions saved to: {output_file}")
