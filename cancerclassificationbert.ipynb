{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11695054,"sourceType":"datasetVersion","datasetId":7340344}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:23.898238Z","iopub.execute_input":"2025-05-06T17:11:23.898873Z","iopub.status.idle":"2025-05-06T17:11:23.904621Z","shell.execute_reply.started":"2025-05-06T17:11:23.898847Z","shell.execute_reply":"2025-05-06T17:11:23.903957Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cancerfinetuning/val.csv\n/kaggle/input/cancerfinetuning/train.csv\n/kaggle/input/cancerfinetuning/test.csv\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/output'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:23.905911Z","iopub.execute_input":"2025-05-06T17:11:23.906409Z","iopub.status.idle":"2025-05-06T17:11:23.915715Z","shell.execute_reply.started":"2025-05-06T17:11:23.906385Z","shell.execute_reply":"2025-05-06T17:11:23.915032Z"}},"outputs":[{"name":"stdout","text":"/kaggle/output/baseline_output/checkpoint-114/trainer_state.json\n/kaggle/output/baseline_output/checkpoint-114/vocab.txt\n/kaggle/output/baseline_output/checkpoint-114/special_tokens_map.json\n/kaggle/output/baseline_output/checkpoint-114/optimizer.pt\n/kaggle/output/baseline_output/checkpoint-114/rng_state.pth\n/kaggle/output/baseline_output/checkpoint-114/training_args.bin\n/kaggle/output/baseline_output/checkpoint-114/config.json\n/kaggle/output/baseline_output/checkpoint-114/tokenizer.json\n/kaggle/output/baseline_output/checkpoint-114/tokenizer_config.json\n/kaggle/output/baseline_output/checkpoint-114/scheduler.pt\n/kaggle/output/baseline_output/checkpoint-114/model.safetensors\n/kaggle/output/baseline_output/checkpoint-38/trainer_state.json\n/kaggle/output/baseline_output/checkpoint-38/vocab.txt\n/kaggle/output/baseline_output/checkpoint-38/special_tokens_map.json\n/kaggle/output/baseline_output/checkpoint-38/optimizer.pt\n/kaggle/output/baseline_output/checkpoint-38/rng_state.pth\n/kaggle/output/baseline_output/checkpoint-38/training_args.bin\n/kaggle/output/baseline_output/checkpoint-38/config.json\n/kaggle/output/baseline_output/checkpoint-38/tokenizer.json\n/kaggle/output/baseline_output/checkpoint-38/tokenizer_config.json\n/kaggle/output/baseline_output/checkpoint-38/scheduler.pt\n/kaggle/output/baseline_output/checkpoint-38/model.safetensors\n/kaggle/output/baseline_output/checkpoint-76/trainer_state.json\n/kaggle/output/baseline_output/checkpoint-76/vocab.txt\n/kaggle/output/baseline_output/checkpoint-76/special_tokens_map.json\n/kaggle/output/baseline_output/checkpoint-76/optimizer.pt\n/kaggle/output/baseline_output/checkpoint-76/rng_state.pth\n/kaggle/output/baseline_output/checkpoint-76/training_args.bin\n/kaggle/output/baseline_output/checkpoint-76/config.json\n/kaggle/output/baseline_output/checkpoint-76/tokenizer.json\n/kaggle/output/baseline_output/checkpoint-76/tokenizer_config.json\n/kaggle/output/baseline_output/checkpoint-76/scheduler.pt\n/kaggle/output/baseline_output/checkpoint-76/model.safetensors\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom transformers import (\n    DistilBertTokenizerFast,\n    DistilBertForSequenceClassification,\n    Trainer, TrainingArguments\n)\nfrom datasets import Dataset\n# import evaluate\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:23.916599Z","iopub.execute_input":"2025-05-06T17:11:23.916823Z","iopub.status.idle":"2025-05-06T17:11:23.927702Z","shell.execute_reply.started":"2025-05-06T17:11:23.916804Z","shell.execute_reply":"2025-05-06T17:11:23.927106Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Load CSVs\ntrain_df = pd.read_csv(\"/kaggle/input/cancerfinetuning/train.csv\")\nval_df = pd.read_csv(\"/kaggle/input/cancerfinetuning/val.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/cancerfinetuning/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:23.929574Z","iopub.execute_input":"2025-05-06T17:11:23.929784Z","iopub.status.idle":"2025-05-06T17:11:23.967139Z","shell.execute_reply.started":"2025-05-06T17:11:23.929769Z","shell.execute_reply":"2025-05-06T17:11:23.966647Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"config = {\n  \"model_name\": \"distilbert-base-uncased\",\n  \"max_length\": 512,\n  \"train_batch_size\": 8,\n  \"eval_batch_size\": 8,\n  \"learning_rate\": 2e-5,\n  \"num_train_epochs\": 3,\n  \"output_dir\": \"/kaggle/working/baseline_output\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:23.967661Z","iopub.execute_input":"2025-05-06T17:11:23.967833Z","iopub.status.idle":"2025-05-06T17:11:23.971472Z","shell.execute_reply.started":"2025-05-06T17:11:23.967819Z","shell.execute_reply":"2025-05-06T17:11:23.970559Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained(config[\"model_name\"])\n\ndef tokenize(batch):\n    return tokenizer(batch['abstract'], padding=\"max_length\", truncation=True, max_length=config[\"max_length\"])\n\ntrain_ds = Dataset.from_pandas(train_df[['abstract', 'label']])\nval_ds = Dataset.from_pandas(val_df[['abstract', 'label']])\ntest_ds = Dataset.from_pandas(test_df[['abstract', 'label']])\n\ntrain_ds = train_ds.map(tokenize, batched=True)\nval_ds = val_ds.map(tokenize, batched=True)\ntest_ds = test_ds.map(tokenize, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:23.972245Z","iopub.execute_input":"2025-05-06T17:11:23.972472Z","iopub.status.idle":"2025-05-06T17:11:24.720376Z","shell.execute_reply.started":"2025-05-06T17:11:23.972454Z","shell.execute_reply":"2025-05-06T17:11:24.719813Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.51.1\",\n  \"vocab_size\": 30522\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbfc1e0a954c4ddeafa2ad75403c3070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bda03d92d444af2a31daf9b8d7bff15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14753dab89e94fc7a96fa45727fdecb1"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"\n# Model\nmodel = DistilBertForSequenceClassification.from_pretrained(config[\"model_name\"], num_labels=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:24.721078Z","iopub.execute_input":"2025-05-06T17:11:24.721323Z","iopub.status.idle":"2025-05-06T17:11:24.894271Z","shell.execute_reply.started":"2025-05-06T17:11:24.721306Z","shell.execute_reply":"2025-05-06T17:11:24.893599Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\nModel config DistilBertConfig {\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForMaskedLM\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.51.1\",\n  \"vocab_size\": 30522\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from transformers import logging\nlogging.set_verbosity_info()  # <-- Enable detailed logs\n\ntraining_args = TrainingArguments(\n    output_dir=config[\"output_dir\"],\n    per_device_train_batch_size=config[\"train_batch_size\"],\n    per_device_eval_batch_size=config[\"eval_batch_size\"],\n    learning_rate=config[\"learning_rate\"],\n    num_train_epochs=config[\"num_train_epochs\"],\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=10,  # Log every 10 steps\n    logging_dir='./logs',\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    report_to=\"none\"  # Disable wandb or other integrations\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:24.894981Z","iopub.execute_input":"2025-05-06T17:11:24.895246Z","iopub.status.idle":"2025-05-06T17:11:24.929201Z","shell.execute_reply.started":"2025-05-06T17:11:24.895228Z","shell.execute_reply":"2025-05-06T17:11:24.928680Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    acc = np.mean(preds == labels)\n    return {\"accuracy\": acc}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:24.929883Z","iopub.execute_input":"2025-05-06T17:11:24.930540Z","iopub.status.idle":"2025-05-06T17:11:24.933884Z","shell.execute_reply.started":"2025-05-06T17:11:24.930516Z","shell.execute_reply":"2025-05-06T17:11:24.933386Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:11:24.935731Z","iopub.execute_input":"2025-05-06T17:11:24.936340Z","iopub.status.idle":"2025-05-06T17:12:31.431644Z","shell.execute_reply.started":"2025-05-06T17:11:24.936318Z","shell.execute_reply":"2025-05-06T17:12:31.431054Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3239434369.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nThe following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 600\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Training with DataParallel so batch size has been adjusted to: 16\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 1\n  Total optimization steps = 114\n  Number of trainable parameters = 66,955,010\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [114/114 01:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.629000</td>\n      <td>0.386884</td>\n      <td>0.940000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.178200</td>\n      <td>0.165375</td>\n      <td>0.945000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.114800</td>\n      <td>0.144650</td>\n      <td>0.955000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 16\nSaving model checkpoint to /kaggle/working/baseline_output/checkpoint-38\nConfiguration saved in /kaggle/working/baseline_output/checkpoint-38/config.json\nModel weights saved in /kaggle/working/baseline_output/checkpoint-38/model.safetensors\ntokenizer config file saved in /kaggle/working/baseline_output/checkpoint-38/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/baseline_output/checkpoint-38/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 16\nSaving model checkpoint to /kaggle/working/baseline_output/checkpoint-76\nConfiguration saved in /kaggle/working/baseline_output/checkpoint-76/config.json\nModel weights saved in /kaggle/working/baseline_output/checkpoint-76/model.safetensors\ntokenizer config file saved in /kaggle/working/baseline_output/checkpoint-76/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/baseline_output/checkpoint-76/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThe following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 16\nSaving model checkpoint to /kaggle/working/baseline_output/checkpoint-114\nConfiguration saved in /kaggle/working/baseline_output/checkpoint-114/config.json\nModel weights saved in /kaggle/working/baseline_output/checkpoint-114/model.safetensors\ntokenizer config file saved in /kaggle/working/baseline_output/checkpoint-114/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/baseline_output/checkpoint-114/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from /kaggle/working/baseline_output/checkpoint-114 (score: 0.955).\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=114, training_loss=0.34004520794801546, metrics={'train_runtime': 65.8703, 'train_samples_per_second': 27.326, 'train_steps_per_second': 1.731, 'total_flos': 238441317580800.0, 'train_loss': 0.34004520794801546, 'epoch': 3.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\nimport openpyxl\nfrom openpyxl.utils.dataframe import dataframe_to_rows\n\n# Summary metrics and confusion matrices\nsummary_records = []\nconf_matrices = {}\n\ndef evaluate_model(split_name, dataset, df, model_tag):\n    preds = trainer.predict(dataset)\n    y_true = preds.label_ids\n    y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    cm = confusion_matrix(y_true, y_pred)\n\n    summary_records.append({\n        \"Model\": model_tag,\n        \"Split\": split_name,\n        \"Accuracy\": f\"{acc*100:.2f}%\",\n        \"F1-score\": f\"{f1:.2f}\"\n    })\n\n    cm_df = pd.DataFrame(\n        cm,\n        index=[\"Actual Cancer\", \"Actual Non-Cancer\"],\n        columns=[\"Predicted Cancer\", \"Predicted Non-Cancer\"]\n    )\n    conf_matrices[f\"{split_name}_{model_tag}\"] = cm_df\n\n# Example: for LoRA model (repeat for baseline)\nevaluate_model(\"train\", train_ds, train_df, \"distilbert\")\nevaluate_model(\"val\", val_ds, val_df, \"distilbert\")\nevaluate_model(\"test\", test_ds, test_df, \"distilbert\")\n\n# Save everything to a single Excel\noutput_path = \"model_performance_report.xlsx\"\nsummary_df = pd.DataFrame(summary_records)\n\nwith pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n    summary_df.to_excel(writer, sheet_name=\"Metrics\", index=False)\n\n    # Create Confusion_Matrix sheet\n    workbook = writer.book\n    worksheet = workbook.create_sheet(\"Confusion_Matrix\")\n\n    for name, cm_df in conf_matrices.items():\n        # Add matrix title\n        worksheet.append([name])  # writes in first column\n        for row in dataframe_to_rows(cm_df, index=True, header=True):\n            worksheet.append(row)\n        worksheet.append([])  # Add empty row between blocks\n\n    # Remove default empty sheet if exists\n    if \"Sheet\" in workbook.sheetnames:\n        del workbook[\"Sheet\"]\n\nprint(f\"✅ All metrics saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T18:12:27.421717Z","iopub.execute_input":"2025-05-06T18:12:27.422027Z","iopub.status.idle":"2025-05-06T18:12:37.768635Z","shell.execute_reply.started":"2025-05-06T18:12:27.422006Z","shell.execute_reply":"2025-05-06T18:12:37.768045Z"}},"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Prediction *****\n  Num examples = 600\n  Batch size = 16\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Prediction *****\n  Num examples = 200\n  Batch size = 16\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Prediction *****\n  Num examples = 200\n  Batch size = 16\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"✅ All metrics saved to model_performance_report.xlsx\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import torch.nn.functional as F\nimport numpy as np\nimport json\n\n# Helper function to generate prediction results\ndef get_predictions(dataset, df, label_map_rev):\n    preds = trainer.predict(dataset)\n    logits = preds.predictions\n    probs = F.softmax(torch.tensor(logits), dim=1).numpy()\n    label_ids = preds.label_ids\n\n    results = []\n    for i, (prob, true_label) in enumerate(zip(probs, label_ids)):\n        pred_label_idx = np.argmax(prob)\n        predicted_labels = [label_map_rev[pred_label_idx]]\n        confidence_scores = {\n            label_map_rev[0]: round(float(prob[0]), 2),\n            label_map_rev[1]: round(float(prob[1]), 2),\n        }\n        results.append({\n            \"id\": str(df.iloc[i]['id']),\n            \"split\": split_name,  # Tag with train/val/test\n            \"predicted_labels\": predicted_labels,\n            \"confidence_scores\": confidence_scores\n        })\n    return results\n\n# Label map\nlabel_map_rev = {0: \"Non-Cancer\", 1: \"Cancer\"}\n\n# Add split name before calling\nsplit_name = \"train\"\ntrain_results = get_predictions(train_ds, train_df.reset_index(drop=True), label_map_rev)\n\nsplit_name = \"val\"\nval_results = get_predictions(val_ds, val_df.reset_index(drop=True), label_map_rev)\n\nsplit_name = \"test\"\ntest_results = get_predictions(test_ds, test_df.reset_index(drop=True), label_map_rev)\n\n# Combine and save\ncombined_results = train_results + val_results + test_results\n\nwith open(\"/kaggle/working/all_predictions.json\", \"w\") as f:\n    json.dump(combined_results, f, indent=2)\n\nprint(\"✅ Saved all_predictions.json with predictions for train, val, and test.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:37:45.323146Z","iopub.execute_input":"2025-05-06T17:37:45.323471Z","iopub.status.idle":"2025-05-06T17:37:55.698383Z","shell.execute_reply.started":"2025-05-06T17:37:45.323451Z","shell.execute_reply":"2025-05-06T17:37:55.697786Z"}},"outputs":[{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Prediction *****\n  Num examples = 600\n  Batch size = 16\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Prediction *****\n  Num examples = 200\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: abstract. If abstract are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Prediction *****\n  Num examples = 200\n  Batch size = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"✅ Saved all_predictions.json with predictions for train, val, and test.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/baseline_output/\")\ntokenizer.save_pretrained(\"/kaggle/working/baseline_output/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:39:47.882211Z","iopub.execute_input":"2025-05-06T17:39:47.882695Z","iopub.status.idle":"2025-05-06T17:39:48.541465Z","shell.execute_reply.started":"2025-05-06T17:39:47.882671Z","shell.execute_reply":"2025-05-06T17:39:48.540855Z"}},"outputs":[{"name":"stderr","text":"Configuration saved in /kaggle/working/baseline_output/config.json\nModel weights saved in /kaggle/working/baseline_output/model.safetensors\ntokenizer config file saved in /kaggle/working/baseline_output/tokenizer_config.json\nSpecial tokens file saved in /kaggle/working/baseline_output/special_tokens_map.json\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/baseline_output/tokenizer_config.json',\n '/kaggle/working/baseline_output/special_tokens_map.json',\n '/kaggle/working/baseline_output/vocab.txt',\n '/kaggle/working/baseline_output/added_tokens.json',\n '/kaggle/working/baseline_output/tokenizer.json')"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"import pickle\n\nwith open(\"/kaggle/working/distilbert_model.pkl\", \"wb\") as f:\n    pickle.dump(model, f)\n\nwith open(\"/kaggle/working/distilbert_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T17:40:41.950229Z","iopub.execute_input":"2025-05-06T17:40:41.950783Z","iopub.status.idle":"2025-05-06T17:40:42.599861Z","shell.execute_reply.started":"2025-05-06T17:40:41.950761Z","shell.execute_reply":"2025-05-06T17:40:42.599104Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}